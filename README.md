# ğŸ•¸ Plataforma de Web Scraping na OLX

Plataforma full-stack que realiza **web scraping automatizado**, armazena os dados coletados em um banco relacional e os exibe em uma **interface web moderna**. O projeto transforma dados brutos da web em informaÃ§Ãµes organizadas e acessÃ­veis ao pÃºblico.

---

## ğŸš€ Tecnologias Utilizadas

### ğŸ“¤ Scraping
- **Linguagem:** Python
- **Framework:** [Playwright](https://playwright.dev/python/)
- **Testes:** pytest
- **Deploy:** GitHub Actions

### ğŸ§  Back-end
- **Framework:** [Spring Boot](https://spring.io/projects/spring-boot)
- **Banco de Dados:** PostgreSQL
- **Testes:** JUnit + Mockito
- **DocumentaÃ§Ã£o da API:** SpringDoc (OpenAPI)
- **Deploy:** Railway

### ğŸ¨ Front-end
- **Framework:** [React.js](https://reactjs.org/)
- **EstilizaÃ§Ã£o:** [Tailwind CSS](https://tailwindcss.com/)
- **Prototipagem:** Figma
- **Testes:**
  - UnitÃ¡rios e de IntegraÃ§Ã£o: Jest + React Testing Library
  - E2E: Playwright
- **Deploy:** Netlify

### ğŸ§± Infraestrutura
- **Containers:** Docker
- **Versionamento:** Git + GitHub

---

## ğŸ›  Funcionalidades

- ğŸ” Coleta automÃ¡tica de dados via scraping (com agendamento)
- ğŸ’¾ Armazenamento dos dados em banco relacional
- ğŸ“¡ ExposiÃ§Ã£o dos dados via API REST
- ğŸŒ VisualizaÃ§Ã£o dos dados em um site responsivo
- âœ… Testes automatizados em todas as camadas

---

## ğŸ“ Estrutura do Projeto

```plaintext
scraper-olx/
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ .gitignore               # Files and folders ignored by Git
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env                     # Environment variables (e.g., USER_AGENT, URL)
â”œâ”€â”€ src/                     # Main source code
â”‚   â”œâ”€â”€ main.py              # Entry point
â”‚   â”œâ”€â”€ config.py            # Centralized configuration
â”‚   â”œâ”€â”€ scrapers/            # One scraper per site
â”‚   â”œâ”€â”€ parsers/             # Functions to extract and clean data from HTML
â”‚   â”œâ”€â”€ storage/             # Persistence layer: save to CSV, JSON, DB, etc.
â”‚   â”œâ”€â”€ utils/               # Helper functions: user-agent, delays, proxies
â”‚   â”œâ”€â”€ models/              # Data structure representation (POPOs or dataclasses)
â”œâ”€â”€ tests/                   # Tests using pytest
â”‚   â””â”€â”€ test_scraper.py      # Unit tests for the scraper
